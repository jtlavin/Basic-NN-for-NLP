{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0XoF085zfFay"
   },
   "source": [
    "### NLP \n",
    "\n",
    "Usando datos de:\n",
    "\n",
    "https://www.kaggle.com/pradeeptrical/text-tweet-classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_excel('text_classification_dataset.xlsx', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to clean the dataset and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow que neceesitamos para usar NN en NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ACNI2012 @TheToka920 Never knew having 1 or 2...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MYCA Magical Moments:\\n\\nSeptember, 2011: Sham...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The current state of last year's @BBL finalist...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@HOLLYJISOO Why did you bring a cricket...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Babar Azam only Pakistani included in the ICC ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    type\n",
       "0  @ACNI2012 @TheToka920 Never knew having 1 or 2...  sports\n",
       "1  MYCA Magical Moments:\\n\\nSeptember, 2011: Sham...  sports\n",
       "2  The current state of last year's @BBL finalist...  sports\n",
       "3         @HOLLYJISOO Why did you bring a cricket...  sports\n",
       "4  Babar Azam only Pakistani included in the ICC ...  sports"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple approach using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tomas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Default stopwords that we would like to delete from our statements\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw.append('')\n",
    "sw.append('tweet')\n",
    "sw.append('tweets')\n",
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleantxt(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+','',text) #quitar menciones\n",
    "    text = re.sub(r'#','',text) #quitar hashtags\n",
    "    text = re.sub(r'RT','',text) #quitar RT\n",
    "    text = re.sub(r'https?:\\/\\/\\S+','',text) #quitar links\n",
    "    \n",
    "    #El resto es para quitar strings que no tienen sentido, las encuentras mirando detenidamente el dataset\n",
    "    \n",
    "    text = re.sub(r'\\n\\n','',text)\n",
    "    text = re.sub(r'\\n','',text)\n",
    "    text = re.sub(r':','',text)\n",
    "    text = re.sub(r'[0-9]','',text)\n",
    "    text = re.sub(r'_[A-Za-z0-9]+','',text)\n",
    "    text = re.sub(r'\\n[A-Za-z0-9]+','',text)\n",
    "    text = re.sub(r'\\'[A-Za-z0-9]+','',text)\n",
    "    text = re.sub(r\"'\",\"\",text)\n",
    "    text = re.sub(r\"...$\",\"\",text)\n",
    "    text = re.sub(r\"..$\",\"\",text)\n",
    "    text = re.sub(r\"....$\",\"\",text)\n",
    "    text = re.sub(r\"...$\",\"\",text)\n",
    "    text = re.sub(r\"`\",\"\",text)\n",
    "    text = re.sub(r\"-\",\"\",text)\n",
    "    text = re.sub(r\"!\",\"\",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Never knew having  or  followers had anythin...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MYCA Magical MomentsSeptember,  Sham Chotoo of...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The current state of last year  finalists    P...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why did you bring</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Babar Azam only Pakistani included in the ICC ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>The senior is one of the most decorated male t...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>COULD be your year to get moving and change t...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>thought you liked yellow on me but that OK....</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>❤️ Tennis greats played together to raise mo...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>A thread on hard court sliding &amp;amp; movemen...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1162 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    type\n",
       "0       Never knew having  or  followers had anythin...  sports\n",
       "1     MYCA Magical MomentsSeptember,  Sham Chotoo of...  sports\n",
       "2     The current state of last year  finalists    P...  sports\n",
       "3                                    Why did you bring   sports\n",
       "4     Babar Azam only Pakistani included in the ICC ...  sports\n",
       "...                                                 ...     ...\n",
       "1157  The senior is one of the most decorated male t...  sports\n",
       "1158   COULD be your year to get moving and change t...  sports\n",
       "1159     thought you liked yellow on me but that OK....  sports\n",
       "1160    ❤️ Tennis greats played together to raise mo...  sports\n",
       "1161    A thread on hard court sliding &amp; movemen...  sports\n",
       "\n",
       "[1162 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limpiamos el texto\n",
    "df['text'] = df['text'].apply(cleantxt)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = []\n",
    "for index in range(df.shape[0]):\n",
    "\n",
    "    text = df.text[index].lower().split(' ')\n",
    "    for word in text:\n",
    "        if word not in sw:\n",
    "            if len(word)>3 and len(word)<9:\n",
    "                new_text.append(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7479"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3615"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_words = set(new_text)\n",
    "len(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_words ={}\n",
    "for word in final_words:\n",
    "    dict_words[word] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chesney': [],\n",
       " 'pick': [],\n",
       " 'less': [],\n",
       " 'sen.': [],\n",
       " 'elbaum': [],\n",
       " 'stealing': [],\n",
       " 'japan': [],\n",
       " 'nearly': [],\n",
       " 'simply': [],\n",
       " 'parnas,': [],\n",
       " 'reason.': [],\n",
       " 'y’all': [],\n",
       " '....📸': [],\n",
       " 'going.': [],\n",
       " 'patsy': [],\n",
       " 'gang..??': [],\n",
       " 'abuse.': [],\n",
       " 'national': [],\n",
       " 'grounds,': [],\n",
       " 'angel': [],\n",
       " 'things': [],\n",
       " 'said,': [],\n",
       " 'appear': [],\n",
       " 'vice': [],\n",
       " 'phillip': [],\n",
       " 'general': [],\n",
       " 'study': [],\n",
       " 'smile': [],\n",
       " '\"every': [],\n",
       " 'wickets': [],\n",
       " 'medvedev': [],\n",
       " 'ahead': [],\n",
       " 'tells': [],\n",
       " 'disabled': [],\n",
       " 'sleeping': [],\n",
       " 'cover,': [],\n",
       " 'flight': [],\n",
       " 'teeth': [],\n",
       " 'streamin': [],\n",
       " 'bowling': [],\n",
       " 'kang': [],\n",
       " 'politic': [],\n",
       " 'band': [],\n",
       " 'accident': [],\n",
       " 'lede': [],\n",
       " 'british': [],\n",
       " 'flawed': [],\n",
       " 'told': [],\n",
       " 'lack,': [],\n",
       " 'happens': [],\n",
       " 'teach': [],\n",
       " 'ugly': [],\n",
       " 'frank': [],\n",
       " 'stay': [],\n",
       " 'britain': [],\n",
       " 'ridge': [],\n",
       " 'bailout': [],\n",
       " 'again.': [],\n",
       " 'dragged': [],\n",
       " 'bully': [],\n",
       " 'dembele,': [],\n",
       " 'gene': [],\n",
       " 'college': [],\n",
       " 'sexism': [],\n",
       " 'sticker': [],\n",
       " 'change?': [],\n",
       " 'goin': [],\n",
       " 'dravid': [],\n",
       " 'ravi': [],\n",
       " 'forward': [],\n",
       " 'guy,': [],\n",
       " 'miami': [],\n",
       " 'innings': [],\n",
       " 'biden?': [],\n",
       " 'nobody': [],\n",
       " '💎/💎today': [],\n",
       " 'virulent': [],\n",
       " 'ukraine': [],\n",
       " 'rock': [],\n",
       " 'greatly': [],\n",
       " 'regional': [],\n",
       " 'winner': [],\n",
       " 'indvaus': [],\n",
       " 'calgary': [],\n",
       " 'reva': [],\n",
       " 'suresh': [],\n",
       " 'bills': [],\n",
       " 'also': [],\n",
       " 'sends': [],\n",
       " 'skort': [],\n",
       " 'caa’': [],\n",
       " 'goodies': [],\n",
       " 'filthy': [],\n",
       " 'dozen': [],\n",
       " 'rape': [],\n",
       " 'yesssss': [],\n",
       " 'kyle': [],\n",
       " 'recently': [],\n",
       " 'praying': [],\n",
       " 'talked': [],\n",
       " 'meetingd': [],\n",
       " 'inks': [],\n",
       " 'drug': [],\n",
       " 'overhead': [],\n",
       " 'ford': [],\n",
       " 'feud': [],\n",
       " 'wrote': [],\n",
       " 'well,': [],\n",
       " 'nonsense': [],\n",
       " 'jumanji': [],\n",
       " 'realized': [],\n",
       " 'meeting': [],\n",
       " 'donated': [],\n",
       " 'creative': [],\n",
       " 'estrogen': [],\n",
       " 'karachi': [],\n",
       " 'exposure': [],\n",
       " 'bryan': [],\n",
       " 'blink': [],\n",
       " 'root': [],\n",
       " 'marnus': [],\n",
       " 'context': [],\n",
       " 'despair': [],\n",
       " '🔥bbl/': [],\n",
       " 'gliding': [],\n",
       " 'asantha': [],\n",
       " 'austr': [],\n",
       " 'aide': [],\n",
       " 'rack': [],\n",
       " 'rights': [],\n",
       " 'masses': [],\n",
       " 'iyer,': [],\n",
       " 'moments': [],\n",
       " 'sees': [],\n",
       " 'foxconn': [],\n",
       " 'spring': [],\n",
       " 'movie': [],\n",
       " 'peopl': [],\n",
       " 'aberdeen': [],\n",
       " 'breaks': [],\n",
       " 'pharma,': [],\n",
       " 'kumar.': [],\n",
       " 'rapping.': [],\n",
       " 'skirt': [],\n",
       " 'scared': [],\n",
       " 'powerful': [],\n",
       " 'budget': [],\n",
       " 'terms': [],\n",
       " 'yearthe': [],\n",
       " 'dep)': [],\n",
       " 'trekkers': [],\n",
       " 'regard': [],\n",
       " 'china': [],\n",
       " 'action': [],\n",
       " 'bitch': [],\n",
       " 'point.': [],\n",
       " 'nrc,': [],\n",
       " 'works?': [],\n",
       " 'dogs': [],\n",
       " 'school.': [],\n",
       " 'behalf': [],\n",
       " 'medical': [],\n",
       " 'tune': [],\n",
       " 'laid': [],\n",
       " 'humor,': [],\n",
       " 'limpopo.': [],\n",
       " 'golden': [],\n",
       " 'trea': [],\n",
       " 'website': [],\n",
       " 'week,': [],\n",
       " 'kickass': [],\n",
       " 'playing': [],\n",
       " 'clean': [],\n",
       " 'manila': [],\n",
       " 'hind': [],\n",
       " '\"all': [],\n",
       " 'train': [],\n",
       " 'loud,': [],\n",
       " 'photo주찬': [],\n",
       " 'mens': [],\n",
       " 'contact': [],\n",
       " 'army': [],\n",
       " ',sis': [],\n",
       " 'suicides': [],\n",
       " 'green': [],\n",
       " 'adidas': [],\n",
       " 'dive': [],\n",
       " 'smiles': [],\n",
       " 'alcohol': [],\n",
       " 'shared': [],\n",
       " 'key.': [],\n",
       " 'you\"': [],\n",
       " 'parking': [],\n",
       " '\"veer': [],\n",
       " 'challen': [],\n",
       " 'directed': [],\n",
       " 'degree,': [],\n",
       " 'book': [],\n",
       " 'chapri': [],\n",
       " 'talkin': [],\n",
       " 'matches': [],\n",
       " 'dispel': [],\n",
       " 'seemed': [],\n",
       " 'caucus': [],\n",
       " 'club': [],\n",
       " 'trek': [],\n",
       " 'topping': [],\n",
       " 'memory': [],\n",
       " 'leather': [],\n",
       " 'towards': [],\n",
       " 'wrong': [],\n",
       " 'backed': [],\n",
       " 'nope.': [],\n",
       " 'kane': [],\n",
       " 'outrage': [],\n",
       " 'issue': [],\n",
       " 'thousand': [],\n",
       " 'family,': [],\n",
       " 'came': [],\n",
       " 'william': [],\n",
       " 'money.': [],\n",
       " 'fort': [],\n",
       " 'released': [],\n",
       " 'knew': [],\n",
       " 'trigger': [],\n",
       " 'rest,': [],\n",
       " 'cover': [],\n",
       " 'founder': [],\n",
       " 'imperial': [],\n",
       " 'hyde?“on': [],\n",
       " 'thro': [],\n",
       " '(almost': [],\n",
       " 'kidssss': [],\n",
       " 'watch': [],\n",
       " 'respond': [],\n",
       " 'saif': [],\n",
       " 'pattas': [],\n",
       " 'always': [],\n",
       " 'texted': [],\n",
       " 'closer': [],\n",
       " 'montreal': [],\n",
       " 'regime': [],\n",
       " 'bhai': [],\n",
       " 'factory': [],\n",
       " 'advoca': [],\n",
       " 'adding': [],\n",
       " 'materi': [],\n",
       " 'age,': [],\n",
       " 'updated': [],\n",
       " 'laureus': [],\n",
       " 'chil': [],\n",
       " 'musical': [],\n",
       " 'asim': [],\n",
       " 'career,': [],\n",
       " 'npr,': [],\n",
       " 'files': [],\n",
       " 'loose': [],\n",
       " 'force': [],\n",
       " 'visually': [],\n",
       " 'whitney': [],\n",
       " 'body’s': [],\n",
       " 'blames': [],\n",
       " 'ago—and': [],\n",
       " 'mink': [],\n",
       " 'harder': [],\n",
       " 'united': [],\n",
       " 'album': [],\n",
       " 'michigan': [],\n",
       " 'nation,': [],\n",
       " 'dude': [],\n",
       " 'rookie': [],\n",
       " 'sharing': [],\n",
       " 'artists': [],\n",
       " 'pundits': [],\n",
       " 'wasn’t': [],\n",
       " 'ago,': [],\n",
       " 'bomb': [],\n",
       " 'favour': [],\n",
       " 'cruz': [],\n",
       " 'trudeau': [],\n",
       " 'develop': [],\n",
       " 'sanders,': [],\n",
       " 'can’t': [],\n",
       " 'relation': [],\n",
       " 'vernon': [],\n",
       " 'john': [],\n",
       " 'leave': [],\n",
       " 'damaging': [],\n",
       " 'thank': [],\n",
       " 'reminder': [],\n",
       " 'we’d': [],\n",
       " 'state.': [],\n",
       " 'rituals': [],\n",
       " 'father’s': [],\n",
       " 'gets': [],\n",
       " 'had...': [],\n",
       " 'allow': [],\n",
       " 'smartly': [],\n",
       " 'sources': [],\n",
       " 'daily': [],\n",
       " 'received': [],\n",
       " 'players': [],\n",
       " 'barred': [],\n",
       " 'netflix.': [],\n",
       " 'actually': [],\n",
       " 'conn.': [],\n",
       " 'jungkook': [],\n",
       " 'elevates': [],\n",
       " 'tell': [],\n",
       " '👏unlike': [],\n",
       " 'cross': [],\n",
       " 'room': [],\n",
       " 'lined': [],\n",
       " 'openly': [],\n",
       " 'pulling': [],\n",
       " 'jooch': [],\n",
       " 'rely': [],\n",
       " 'data': [],\n",
       " 'shreyas': [],\n",
       " 'problems': [],\n",
       " 'solo': [],\n",
       " 'asking': [],\n",
       " 'endgame': [],\n",
       " 'signing': [],\n",
       " 'giuliani': [],\n",
       " 'trebek': [],\n",
       " 'hipaa': [],\n",
       " 'license': [],\n",
       " 'quebec': [],\n",
       " 'theory': [],\n",
       " 'we’re': [],\n",
       " 'チャニョル': [],\n",
       " 'meal': [],\n",
       " 'narr': [],\n",
       " 'gamecle': [],\n",
       " 'episode': [],\n",
       " 'day..': [],\n",
       " 'audienc': [],\n",
       " 'agencies': [],\n",
       " 'chew': [],\n",
       " 'senior': [],\n",
       " 'oddly': [],\n",
       " 'century': [],\n",
       " 'answer': [],\n",
       " 'erro': [],\n",
       " 'imagine': [],\n",
       " 'quint': [],\n",
       " 'prepping': [],\n",
       " 'bags': [],\n",
       " 'amu.': [],\n",
       " 'dark': [],\n",
       " 'sir.': [],\n",
       " 'ouster,': [],\n",
       " 'ecareers': [],\n",
       " 'vision,': [],\n",
       " '(known': [],\n",
       " 'stupid.': [],\n",
       " 'lovely': [],\n",
       " 'doctors.': [],\n",
       " 'pills,': [],\n",
       " 'security': [],\n",
       " 'info\"the': [],\n",
       " 'movies': [],\n",
       " 'test': [],\n",
       " 'better': [],\n",
       " 'premier': [],\n",
       " 'plea,': [],\n",
       " 'moves': [],\n",
       " 'tips': [],\n",
       " 'prime': [],\n",
       " 'valid': [],\n",
       " 'housing': [],\n",
       " 'medi': [],\n",
       " 'give': [],\n",
       " 'phones': [],\n",
       " 'talks': [],\n",
       " 'departed': [],\n",
       " 'dead': [],\n",
       " 'bathing': [],\n",
       " 'credible': [],\n",
       " 'wasim': [],\n",
       " 'changing': [],\n",
       " 'backlash': [],\n",
       " 'secure': [],\n",
       " 'merch': [],\n",
       " 'crisis': [],\n",
       " 'jewel': [],\n",
       " 'beneath': [],\n",
       " 'lefty': [],\n",
       " '\"curse': [],\n",
       " 'case': [],\n",
       " 'poli': [],\n",
       " 'nawaz': [],\n",
       " 'toxi': [],\n",
       " 'king': [],\n",
       " 'coming': [],\n",
       " 'good.': [],\n",
       " 'resource': [],\n",
       " 'thrown': [],\n",
       " 'persona': [],\n",
       " 'keeping': [],\n",
       " 'outreach': [],\n",
       " 'worry': [],\n",
       " 'racism': [],\n",
       " 'roasted': [],\n",
       " 'cricket': [],\n",
       " 'highest': [],\n",
       " 'they’re': [],\n",
       " 'want.': [],\n",
       " 'rule': [],\n",
       " 'runs': [],\n",
       " 'ring': [],\n",
       " 'here”': [],\n",
       " 'focusing': [],\n",
       " 'rejected': [],\n",
       " 'ought': [],\n",
       " '🏏apart': [],\n",
       " 'spirit': [],\n",
       " 'life.': [],\n",
       " 'shooting': [],\n",
       " 'newswho': [],\n",
       " 'goiz': [],\n",
       " 'spiked': [],\n",
       " 'package,': [],\n",
       " 'worst': [],\n",
       " 'citizen': [],\n",
       " 'kurien': [],\n",
       " 'poorer.': [],\n",
       " 'shaved': [],\n",
       " 'arena': [],\n",
       " 'source': [],\n",
       " 'families': [],\n",
       " 'grants': [],\n",
       " 'we’ll': [],\n",
       " 'hammer': [],\n",
       " 'woman': [],\n",
       " 'won😀': [],\n",
       " 'yemen': [],\n",
       " 'debt': [],\n",
       " 'began': [],\n",
       " 'pass': [],\n",
       " 'asifjo': [],\n",
       " 'flawless': [],\n",
       " 'event.': [],\n",
       " 'nominees': [],\n",
       " 'several': [],\n",
       " 'cummins': [],\n",
       " 'soon': [],\n",
       " 'soccer': [],\n",
       " 'mystery': [],\n",
       " 'danilla': [],\n",
       " 'groups': [],\n",
       " 'diabetes': [],\n",
       " 'livers': [],\n",
       " 'resister': [],\n",
       " 'outsi': [],\n",
       " 'apply': [],\n",
       " 'leo,': [],\n",
       " 'using': [],\n",
       " 'roar': [],\n",
       " 'found': [],\n",
       " 'poorest': [],\n",
       " 'sirius': [],\n",
       " 'isnt': [],\n",
       " 'firsta': [],\n",
       " 'guys': [],\n",
       " 'absolute': [],\n",
       " 'biggest': [],\n",
       " 'informal': [],\n",
       " 'educate': [],\n",
       " 'cute': [],\n",
       " 'hyped': [],\n",
       " 'play': [],\n",
       " 'louis': [],\n",
       " 'standing': [],\n",
       " 'smart': [],\n",
       " 'cuts,': [],\n",
       " 'draw.': [],\n",
       " 'victims': [],\n",
       " '🥺💔it': [],\n",
       " 'denis': [],\n",
       " 'info': [],\n",
       " 'growing': [],\n",
       " 'bills?': [],\n",
       " 'jones,': [],\n",
       " 'east': [],\n",
       " 'credit': [],\n",
       " 'aides,': [],\n",
       " 'longtime': [],\n",
       " 'shot': [],\n",
       " 'compl': [],\n",
       " '“you’re': [],\n",
       " 'g.c.': [],\n",
       " 'member': [],\n",
       " 'pompeo': [],\n",
       " 'safety,': [],\n",
       " 'pandey': [],\n",
       " 'starter': [],\n",
       " 'serena': [],\n",
       " '&amp;': [],\n",
       " 'person.': [],\n",
       " 'finance': [],\n",
       " 'wrench': [],\n",
       " 'largely': [],\n",
       " 'heard': [],\n",
       " 'helped': [],\n",
       " 'farm': [],\n",
       " 'contract': [],\n",
       " 'liberals': [],\n",
       " '👀robert': [],\n",
       " 'poll': [],\n",
       " 'machine': [],\n",
       " 'paid': [],\n",
       " 'giving': [],\n",
       " 'debate': [],\n",
       " 'prior': [],\n",
       " 'orders': [],\n",
       " 'russia': [],\n",
       " 'send': [],\n",
       " 'clash': [],\n",
       " 'forever': [],\n",
       " 'party': [],\n",
       " 'obama,': [],\n",
       " 'grateful': [],\n",
       " 'vince': [],\n",
       " 'manne': [],\n",
       " 'kids.': [],\n",
       " 'douglas,': [],\n",
       " 'debut': [],\n",
       " 'exceed': [],\n",
       " 'russians': [],\n",
       " 'links': [],\n",
       " 'match': [],\n",
       " 'proposed': [],\n",
       " 'someone?': [],\n",
       " 'lea,': [],\n",
       " 'gone': [],\n",
       " 'feels': [],\n",
       " 'worker,': [],\n",
       " 'massive': [],\n",
       " 'zuma’s': [],\n",
       " 'publicad': [],\n",
       " 'won’t': [],\n",
       " 'wild': [],\n",
       " 'shirt': [],\n",
       " 'attached': [],\n",
       " 'demo': [],\n",
       " 'whatsoev': [],\n",
       " 'ayushma': [],\n",
       " 'shitty': [],\n",
       " 'roaming': [],\n",
       " 'auckland': [],\n",
       " 'corbyn': [],\n",
       " 'field': [],\n",
       " 'complex;': [],\n",
       " 'sort': [],\n",
       " 'vote': [],\n",
       " 'lloyd': [],\n",
       " 'israeli': [],\n",
       " 'manish': [],\n",
       " 'shift': [],\n",
       " 'godzilla': [],\n",
       " 'advice': [],\n",
       " 'lawyers': [],\n",
       " 'sing': [],\n",
       " 'warns': [],\n",
       " 'berlin': [],\n",
       " 'steals': [],\n",
       " 'david': [],\n",
       " 'often': [],\n",
       " 'shames': [],\n",
       " 'sues': [],\n",
       " 'beautifu': [],\n",
       " 'patients': [],\n",
       " 'southern': [],\n",
       " 'auction': [],\n",
       " 'kathyrn': [],\n",
       " 'deny,': [],\n",
       " 'post': [],\n",
       " 'galaxya': [],\n",
       " 'spurs*': [],\n",
       " 'concept': [],\n",
       " 'welcome': [],\n",
       " 'stage': [],\n",
       " 'covered': [],\n",
       " 'seungkw': [],\n",
       " 'issues.': [],\n",
       " 'notice': [],\n",
       " 'bawling': [],\n",
       " 'steyer': [],\n",
       " 'exo’s': [],\n",
       " 'piece': [],\n",
       " 'princess': [],\n",
       " 'device': [],\n",
       " 'police': [],\n",
       " 'insys': [],\n",
       " 'city': [],\n",
       " 'humanity': [],\n",
       " 'vague': [],\n",
       " 'seeking': [],\n",
       " 'america': [],\n",
       " 'contest': [],\n",
       " 'souls”': [],\n",
       " 'smashing': [],\n",
       " 'one.': [],\n",
       " 'iran\"': [],\n",
       " 'paints': [],\n",
       " 'games': [],\n",
       " 'sohu': [],\n",
       " 'toyota': [],\n",
       " 'dealing': [],\n",
       " 'forward,': [],\n",
       " '\"sally': [],\n",
       " 'provided': [],\n",
       " 'admitted': [],\n",
       " 'abi,': [],\n",
       " 'motto,': [],\n",
       " 'plun': [],\n",
       " 'safdar': [],\n",
       " 'mobstyle': [],\n",
       " 'shame': [],\n",
       " 'war.': [],\n",
       " 'surfaced': [],\n",
       " 'isner': [],\n",
       " 'tomas': [],\n",
       " 'desis': [],\n",
       " 'yoongi': [],\n",
       " 'texts.': [],\n",
       " 'male': [],\n",
       " 'november': [],\n",
       " 'dead.': [],\n",
       " 'safe': [],\n",
       " 'remarks': [],\n",
       " 'appro': [],\n",
       " 'leader,': [],\n",
       " 'past': [],\n",
       " 'pursued': [],\n",
       " 'double': [],\n",
       " 'academic': [],\n",
       " 'clamps': [],\n",
       " 'dying': [],\n",
       " 'shadow,': [],\n",
       " 'prepared': [],\n",
       " 'means': [],\n",
       " '😂😂oh': [],\n",
       " 'whil': [],\n",
       " 'divide': [],\n",
       " 'rallies': [],\n",
       " 'conway': [],\n",
       " 'makers': [],\n",
       " 'changed': [],\n",
       " 'russian': [],\n",
       " 'movie?': [],\n",
       " 'conned.': [],\n",
       " 'india’s': [],\n",
       " 'hold': [],\n",
       " 'gift': [],\n",
       " 'thri': [],\n",
       " 'brand': [],\n",
       " 'light': [],\n",
       " 'knife': [],\n",
       " 'senator,': [],\n",
       " 'cats': [],\n",
       " 'years.': [],\n",
       " 'concerns': [],\n",
       " 'employer': [],\n",
       " 'wants,': [],\n",
       " 'roll': [],\n",
       " 'activato': [],\n",
       " 'wanting': [],\n",
       " 'hearing': [],\n",
       " 'fedal': [],\n",
       " 'literal': [],\n",
       " 'started': [],\n",
       " 'seen.': [],\n",
       " 'looming,': [],\n",
       " 'beit': [],\n",
       " 'liked': [],\n",
       " 'agenda': [],\n",
       " '“for': [],\n",
       " 'quick': [],\n",
       " 'osler,': [],\n",
       " 'wisdom': [],\n",
       " 'indvsaus': [],\n",
       " 'mountain': [],\n",
       " 'treasure': [],\n",
       " 'grade': [],\n",
       " 'talking': [],\n",
       " 'i’ll': [],\n",
       " 'tourists': [],\n",
       " 'hand,': [],\n",
       " 'nike': [],\n",
       " 'said': [],\n",
       " 'keep': [],\n",
       " 'fairly': [],\n",
       " 'could': [],\n",
       " 'gurney': [],\n",
       " 'bigger': [],\n",
       " 'metals': [],\n",
       " 'renewing': [],\n",
       " 'corpsmaj': [],\n",
       " 'paper.': [],\n",
       " 'olympic': [],\n",
       " 'monsta': [],\n",
       " 'history.': [],\n",
       " 'cric': [],\n",
       " 'retweet': [],\n",
       " 'openings': [],\n",
       " 'for..': [],\n",
       " 'insights': [],\n",
       " 'booth': [],\n",
       " 'like.': [],\n",
       " 'profits': [],\n",
       " 'pretty': [],\n",
       " 'back': [],\n",
       " 'kind': [],\n",
       " 'ticket': [],\n",
       " 'kicking': [],\n",
       " 'reading': [],\n",
       " 'period.': [],\n",
       " '\"ode': [],\n",
       " 'seem': [],\n",
       " 'relive': [],\n",
       " 'behave': [],\n",
       " 'two.': [],\n",
       " 'house': [],\n",
       " 'lots': [],\n",
       " 'kindly': [],\n",
       " 'tiktok': [],\n",
       " 'coat': [],\n",
       " 'empty?': [],\n",
       " 'comfort': [],\n",
       " 'pays': [],\n",
       " 'module': [],\n",
       " 'gadgets\"': [],\n",
       " 'senators': [],\n",
       " 'unnerved': [],\n",
       " 'qualms': [],\n",
       " 'appears': [],\n",
       " 'stories': [],\n",
       " 'rather': [],\n",
       " 'afternoo': [],\n",
       " 'usual': [],\n",
       " 'maybe': [],\n",
       " 'picked': [],\n",
       " 'open.': [],\n",
       " 'given': [],\n",
       " 'wrong”': [],\n",
       " 'risin': [],\n",
       " 'confirm': [],\n",
       " 'taylor.': [],\n",
       " 'driving': [],\n",
       " 'hitman': [],\n",
       " 'stays': [],\n",
       " 'training': [],\n",
       " 'giroud': [],\n",
       " 'legends': [],\n",
       " 'school': [],\n",
       " 'hong': [],\n",
       " 'ac/dc': [],\n",
       " 'naomi': [],\n",
       " 'ponna': [],\n",
       " '“prove,': [],\n",
       " 'solve': [],\n",
       " 'although': [],\n",
       " 'rajkot': [],\n",
       " 'still': [],\n",
       " 'shafi,': [],\n",
       " 'narrator': [],\n",
       " 'lights': [],\n",
       " 'spend': [],\n",
       " 'join': [],\n",
       " 'physical': [],\n",
       " 'con?': [],\n",
       " 'used': [],\n",
       " 'suggest': [],\n",
       " 'enough': [],\n",
       " 'season': [],\n",
       " 'titled': [],\n",
       " '(doctor)': [],\n",
       " 'who’s': [],\n",
       " 'fognini': [],\n",
       " 'tavleen': [],\n",
       " 'watched': [],\n",
       " 'kids': [],\n",
       " 'circle': [],\n",
       " 'five': [],\n",
       " 'pack': [],\n",
       " 'toll': [],\n",
       " 'exactly.': [],\n",
       " 'interest': [],\n",
       " 'words': [],\n",
       " 'variety': [],\n",
       " 'stivo': [],\n",
       " 'shorts': [],\n",
       " 'regroup': [],\n",
       " 'presence': [],\n",
       " 'clerica': [],\n",
       " 'attorney': [],\n",
       " '🥰nice': [],\n",
       " 'story': [],\n",
       " 'proving': [],\n",
       " 'africa': [],\n",
       " 'save': [],\n",
       " 'medium': [],\n",
       " 'reading.': [],\n",
       " 'cant': [],\n",
       " 'decent': [],\n",
       " 'trump’s': [],\n",
       " 'texans': [],\n",
       " 'whoever': [],\n",
       " 'u.s.': [],\n",
       " 'awaited': [],\n",
       " 'natural': [],\n",
       " 'leadersh': [],\n",
       " 'mrs.': [],\n",
       " 'plan': [],\n",
       " 'league': [],\n",
       " 'stated': [],\n",
       " 'internal': [],\n",
       " 'grew': [],\n",
       " 'thing': [],\n",
       " 'meps🇩🇪': [],\n",
       " 'chats': [],\n",
       " 'american': [],\n",
       " 'tweeted': [],\n",
       " 'flynn': [],\n",
       " 'litt': [],\n",
       " 'bunch': [],\n",
       " 'hearts': [],\n",
       " 'fanchant': [],\n",
       " 'brings': [],\n",
       " 'trucks': [],\n",
       " 'activ': [],\n",
       " 'idea': [],\n",
       " 'sweetie”': [],\n",
       " 'teams': [],\n",
       " 'pfc.': [],\n",
       " 'guitar': [],\n",
       " 'indian?’': [],\n",
       " 'millions': [],\n",
       " 'chapter.': [],\n",
       " 'concert😫': [],\n",
       " 'laughter': [],\n",
       " 'utah,': [],\n",
       " 'version': [],\n",
       " '(still': [],\n",
       " 'gone,': [],\n",
       " 'educ': [],\n",
       " 'irrespon': [],\n",
       " 'escort': [],\n",
       " 'ready': [],\n",
       " 'roger.': [],\n",
       " 'flori': [],\n",
       " 'kenyatta': [],\n",
       " 'kathniel': [],\n",
       " 'night,': [],\n",
       " 'mommy': [],\n",
       " 'raise': [],\n",
       " 'bank': [],\n",
       " 'stray': [],\n",
       " 'edition': [],\n",
       " 'eric': [],\n",
       " 'worth': [],\n",
       " 'performi': [],\n",
       " 'canadian': [],\n",
       " 'fund': [],\n",
       " 'health': [],\n",
       " 'intimate': [],\n",
       " 'prog': [],\n",
       " 'showed': [],\n",
       " 'cun,': [],\n",
       " 'newly': [],\n",
       " 'but,': [],\n",
       " 'famous': [],\n",
       " 'seige': [],\n",
       " 'failed': [],\n",
       " 'europe': [],\n",
       " 'boss': [],\n",
       " 'ariana': [],\n",
       " 'zero': [],\n",
       " 'husbands': [],\n",
       " 'demand’': [],\n",
       " '’species': [],\n",
       " 'likely': [],\n",
       " 'adam': [],\n",
       " 'blair': [],\n",
       " 'texas’': [],\n",
       " 'singer,': [],\n",
       " 'seems': [],\n",
       " 'hacking': [],\n",
       " 'mark': [],\n",
       " 'dtcle,': [],\n",
       " 'sold.': [],\n",
       " 'truly': [],\n",
       " 'glows': [],\n",
       " 'damn,': [],\n",
       " 'reflects': [],\n",
       " 'start': [],\n",
       " 'night': [],\n",
       " 'lane': [],\n",
       " 'thejoker': [],\n",
       " 'job;': [],\n",
       " 'host': [],\n",
       " 'shri': [],\n",
       " 'former': [],\n",
       " 'hater': [],\n",
       " 'spoiler': [],\n",
       " 'botham,': [],\n",
       " 'festival': [],\n",
       " 'vintage': [],\n",
       " 'reports': [],\n",
       " 'charges,': [],\n",
       " 'gesture': [],\n",
       " 'segment': [],\n",
       " 'proj': [],\n",
       " 'trashing': [],\n",
       " 'awarded': [],\n",
       " 'fulls': [],\n",
       " 'gotta': [],\n",
       " 'mater': [],\n",
       " 'expected': [],\n",
       " 'recent': [],\n",
       " 'aged': [],\n",
       " 'shameful': [],\n",
       " 'nick': [],\n",
       " 'teamin': [],\n",
       " 'deeply': [],\n",
       " 'anothe': [],\n",
       " 'witness,': [],\n",
       " 'alex': [],\n",
       " 'sexist': [],\n",
       " 'akye': [],\n",
       " 'minister': [],\n",
       " 'state,': [],\n",
       " 'music': [],\n",
       " 'numerous': [],\n",
       " 'nader,': [],\n",
       " 'learning': [],\n",
       " 'yet.': [],\n",
       " 'unbiased': [],\n",
       " 'roasting': [],\n",
       " 'bunco': [],\n",
       " 'trial.': [],\n",
       " 'worship': [],\n",
       " 'insurers': [],\n",
       " 'seokmin': [],\n",
       " 'unlikely': [],\n",
       " 'bcoz': [],\n",
       " 'liar,': [],\n",
       " 'perfectl': [],\n",
       " 'feel': [],\n",
       " 'informed': [],\n",
       " 'claimed': [],\n",
       " 'serious': [],\n",
       " 'rise': [],\n",
       " 'drama': [],\n",
       " 'athletes': [],\n",
       " 'clinton': [],\n",
       " 'justice': [],\n",
       " 'longer': [],\n",
       " 'kaweek': [],\n",
       " 'issues,': [],\n",
       " 'puff': [],\n",
       " 'comes': [],\n",
       " 'deep': [],\n",
       " 'hardware': [],\n",
       " 'haha': [],\n",
       " 'jongdae': [],\n",
       " 'favor': [],\n",
       " 'inosuke': [],\n",
       " 'kriswu': [],\n",
       " 'wheeeee': [],\n",
       " 'masse,': [],\n",
       " 'haven’t': [],\n",
       " 'far.': [],\n",
       " 'elected': [],\n",
       " 'strongly': [],\n",
       " 'yellow': [],\n",
       " '(along': [],\n",
       " 'grossing': [],\n",
       " 'movie?me': [],\n",
       " 'lalahaji': [],\n",
       " 'pakistan': [],\n",
       " 'third': [],\n",
       " 'sign': [],\n",
       " 'cebu': [],\n",
       " 'federal': [],\n",
       " 'explain': [],\n",
       " 'lied.': [],\n",
       " 'babe,': [],\n",
       " 'passes': [],\n",
       " 'click': [],\n",
       " 'awesome.': [],\n",
       " 'murder': [],\n",
       " 'betting': [],\n",
       " 'chahar': [],\n",
       " 'visited': [],\n",
       " 'outraged': [],\n",
       " 'maligned': [],\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(df.shape[0]):\n",
    "    sentence = df.text[index].lower().split(' ')\n",
    "    for word in final_words:\n",
    "        if word in sentence:\n",
    "            n_times = sentence.count(word)\n",
    "            dict_words[word].append(n_times)\n",
    "        else:\n",
    "            dict_words[word].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1162, 3615)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words = pd.DataFrame(dict_words)\n",
    "df_words.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Máximo del Dataframe para normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = df_words.to_numpy().max()\n",
    "df_words = df_words/max_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1162, 3615)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_words.to_numpy(float)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_rep = {'sports':1,\n",
    "           'medical':2,\n",
    "           'entertainment':3,\n",
    "           'politics':4}\n",
    "y = df.type\n",
    "y = y.replace(dict_rep)\n",
    "y = np.array(y).astype(int)\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainingX,testX,trainingy,testy = train_test_split(X,y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871, 3615) (291, 3615) (871,) (291,)\n"
     ]
    }
   ],
   "source": [
    "print(trainingX.shape,testX.shape,trainingy.shape, testy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "Epoch 1/6\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5790 - accuracy: 0.3628\n",
      "Epoch 2/6\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.4709 - accuracy: 0.5339\n",
      "Epoch 3/6\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.3254 - accuracy: 0.6900\n",
      "Epoch 4/6\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.1534 - accuracy: 0.8060\n",
      "Epoch 5/6\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.9695 - accuracy: 0.8760\n",
      "Epoch 6/6\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.7917 - accuracy: 0.9495\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 1.0696 - accuracy: 0.6907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.069620132446289, 0.6907216310501099]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(5, activation=tf.nn.softmax)])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(trainingX, trainingy, epochs=6, verbose=1)\n",
    "\n",
    "model.evaluate(testX, testy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "house_pricing.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
